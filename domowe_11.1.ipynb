{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MaksymilianSzymczak/ML24/blob/main/domowe_11.1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Homework Assignment: Understanding Deconvolution in Autoencoders**\n",
        "---------------\n",
        "\n",
        "In class, we worked with autoencoders built from multilayer perceptrons (MLPs). However, encoders are often constructed using convolutional architectures to better capture spatial patterns. In this assignment, you'll explore how the decoder can use deconvolutional (transposed convolution) layers to reverse and mirror the operations performed by the convolutional encoder.\n",
        "\n",
        "While convolutional encoders are relatively well understood, **decoding (or upsampling) the compressed representation** using **deconvolutional layers** (also known as **transposed convolutions**) often raises questions.\n",
        "\n",
        "This assignment is particularly relevant because deconvolution is a core component of the U-Net architecture, a prominent neural network used extensively in image segmentation tasks.\n",
        "\n",
        "Your main objective is to deeply understand **how transposed convolution layers work**, and explain them in both words and visuals.\n",
        "\n",
        "\n",
        "## **The Objective**\n",
        "\n",
        "Understand and clearly explain how **transposed convolutions** work. Use 2D transposed convolutions and a small grid of 2D points as a working example.\n",
        "\n",
        "You may need to do some additional reading to complete this assignment.\n",
        "\n",
        "## **Tasks & Deliverables**\n",
        "\n",
        "### 1. **Theory Exploration**\n",
        "\n",
        "Using markdown cells in your Colab notebook, answer the following:\n",
        "\n",
        "- What is a **transposed convolution**?\n",
        "- How does it differ from a regular convolution?\n",
        "- How does it upsample feature maps?\n",
        "- What are **stride**, **padding**, and **kernel size**, and how do they influence the result in a transposed convolution?\n",
        "- To earn full two points, your explanation must be detailed enough for a reader to reproduce the upsampling process step by step.\n",
        "\n",
        "\n",
        "### 2. **Manual Diagram (by your hand, not a generated image)**\n",
        "\n",
        "Carefully plan and draw **by hand** a diagram or a set of diagrams that:\n",
        "\n",
        "- Explain the process of using **transposed convolution**.\n",
        "- Use an example of a **small input grid of 2D points** which gets expanded into a larger output grid.\n",
        "- Explain how stride, padding, and the kernel shape affect the result.\n",
        "- Show intermediate steps of the operation, not just input and output.\n",
        "\n",
        "**Scan or photograph your diagram(s)**, and upload it to your **GitHub repository** for this course.\n",
        "\n",
        "Then embed it in your Colab notebook using markdown (you can find examples on *how to do it* in previous notebooks related to this class, e.g. the one on linear regression or the one on the MLP network).\n",
        "\n",
        "\n",
        "### 3. **Publish on GitHub**  \n",
        "   - Place the Colab notebook in your **GitHub repository** for this course.\n",
        "   - In your repository’s **README**, add a **link** to the notebook and also include an **“Open in Colab”** badge at the top of the notebook so it can be launched directly from GitHub.\n"
      ],
      "metadata": {
        "id": "4Ar3Nf6gOX4Y"
      },
      "id": "4Ar3Nf6gOX4Y"
    },
    {
      "cell_type": "markdown",
      "source": [
        "What is a transposed convolution?\n",
        "\n",
        "Transponowana konwolucja (czyli transposed convolution) to taki trik w sieciach neuronowych, który pozwala nam „odwrócić” zwykłą konwolucję i powiększyć obrazek albo mapę cech do większego rozmiaru. Zamiast ściskać dane jak w klasycznej konwolucji, tutaj każdy piksel wejścia jest „rozsiewany” na większą siatkę za pomocą filtra, a tam gdzie się te rozsiane wartości nakładają, po prostu się sumują. Dzięki temu możemy np. z małej, skompresowanej reprezentacji „odbudować” większy obrazek, co jest bardzo przydatne w dekoderach autoenkoderów czy sieciach typu U-Net do segmentacji obrazów. To nie jest dokładne cofnięcie konwolucji, bo pewne informacje giną w procesie ściskania, ale pozwala na sensowne powiększanie i rekonstrukcję danych. W praktyce transponowana konwolucja działa trochę jak „inteligentne rozciąganie” obrazu, gdzie sieć sama uczy się, jak najlepiej rozmieścić szczegóły na powiększonym wyjściu\n",
        "\n",
        "\n",
        "\n",
        "How does it differ from a regular convolution?\n",
        "\n",
        "Transponowana konwolucja, czyli dekonwolucja, powiększa (upsampluje) mapy cech w taki sposób, że bierze małą macierz i „rozciąga” ją do większego rozmiaru. Najpierw między wartościami wejściowymi wstawia zera (w zależności od stride), przez co powstaje większa siatka z wieloma zerami. Następnie na tę nową siatkę nakłada filtr (kernel), przesuwając go po całej macierzy i sumując wyniki tam, gdzie się nakładają. Dzięki temu każdy piksel wejścia „rozlewa się” na większy obszar wyjścia, a sieć może nauczyć się, jak najlepiej odtworzyć szczegóły obrazu podczas powiększania. W skrócie: transponowana konwolucja pozwala sieci zamienić skompresowaną, małą mapę cech z powrotem w większy, bardziej szczegółowy obraz"
      ],
      "metadata": {
        "id": "fz42MRkcLOU2"
      },
      "id": "fz42MRkcLOU2"
    },
    {
      "cell_type": "markdown",
      "source": [
        "How does it upsample feature maps?\n",
        "\n",
        "Oto przykład przeprowadzenia transponowanej konwolucji krok po kroku na innych wartościach niż w poprzednim opisie:\n",
        "\n",
        "Załóżmy, że mamy macierz wejściową:\n",
        "\n",
        "$$\n",
        "\\begin{bmatrix}\n",
        "5 & 1 \\\\\n",
        "2 & 3 \\\\\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "Chcemy ją powiększyć za pomocą transponowanej konwolucji z kernelem 2x2 o wartościach:\n",
        "\n",
        "$$\n",
        "\\begin{bmatrix}\n",
        "1 & 0 \\\\\n",
        "0 & -1 \\\\\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "Stride ustawiamy na 2, padding na 0.\n",
        "\n",
        "**1. Wstawianie zer (upsampling przez stride):**\n",
        "\n",
        "Dla stride = 2, pomiędzy wartościami i wierszami wstawiamy po jednym zerze:\n",
        "\n",
        "$$\n",
        "\\begin{bmatrix}\n",
        "5 & 0 & 1 \\\\\n",
        "0 & 0 & 0 \\\\\n",
        "2 & 0 & 3 \\\\\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "**2. Przygotowanie macierzy wyjściowej:**\n",
        "\n",
        "Rozmiar wyjściowy obliczamy jako:  \n",
        "$$\n",
        "n = stride \\times (input\\_size - 1) + kernel\\_size\n",
        "$$\n",
        "Dla naszego przykładu:  \n",
        "$$\n",
        "n = 2 \\times (2 - 1) + 2 = 4\n",
        "$$\n",
        "Tworzymy pustą macierz 4x4 wypełnioną zerami.\n",
        "\n",
        "**3. Nakładanie kernela na niezerowe pozycje:**\n",
        "\n",
        "Przechodzimy przez macierz po upsamplingu i dla każdej niezerowej wartości nakładamy kernel, mnożąc go przez tę wartość i sumując wyniki w odpowiednich miejscach macierzy wyjściowej.\n",
        "\n",
        "- Pozycja (0,0): wartość 5  \n",
        "Nakładamy kernel pomnożony przez 5 na pozycję (0,0):\n",
        "\n",
        "$$\n",
        "\\begin{bmatrix}\n",
        "5 & 0 \\\\\n",
        "0 & -5 \\\\\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "Dodajemy do macierzy wyjściowej w lewym górnym rogu.\n",
        "\n",
        "- Pozycja (0,2): wartość 1  \n",
        "Kernel * 1, pozycja (0,2):\n",
        "\n",
        "$$\n",
        "\\begin{bmatrix}\n",
        "1 & 0 \\\\\n",
        "0 & -1 \\\\\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "Dodajemy do macierzy wyjściowej od pozycji (0,2).\n",
        "\n",
        "- Pozycja (2,0): wartość 2  \n",
        "Kernel * 2, pozycja (2,0):\n",
        "\n",
        "$$\n",
        "\\begin{bmatrix}\n",
        "2 & 0 \\\\\n",
        "0 & -2 \\\\\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "Dodajemy od pozycji (2,0).\n",
        "\n",
        "- Pozycja (2,2): wartość 3  \n",
        "Kernel * 3, pozycja (2,2):\n",
        "\n",
        "$$\n",
        "\\begin{bmatrix}\n",
        "3 & 0 \\\\\n",
        "0 & -3 \\\\\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "Dodajemy od pozycji (2,2).\n",
        "\n",
        "**4. Sumowanie nakładających się wartości:**\n",
        "\n",
        "Po dodaniu wszystkich wyników, macierz wyjściowa wygląda tak:\n",
        "\n",
        "$$\n",
        "\\begin{bmatrix}\n",
        "5 & 0 & 1 & 0 \\\\\n",
        "0 & -5 & 0 & -1 \\\\\n",
        "2 & 0 & 3 & 0 \\\\\n",
        "0 & -2 & 0 & -3 \\\\\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "W ten sposób uzyskaliśmy powiększoną mapę cech za pomocą transponowanej konwolucji — każdy element wejścia „rozlał się” po większej macierzy zgodnie z kernelem, a tam gdzie wartości się nakładały, zostały zsumowane.\n"
      ],
      "metadata": {
        "id": "8JCTP-GBJ5M6"
      },
      "id": "8JCTP-GBJ5M6"
    },
    {
      "cell_type": "markdown",
      "source": [
        "What are stride, padding, and kernel size, and how do they influence the result in a transposed convolution?\n",
        "\n",
        "Stride, padding i kernel size to trzy podstawowe parametry, które decydują o tym, jak transponowana konwolucja powiększa mapy cech. Stride (krok) określa, jak bardzo „rozciągamy” dane wejściowe — im większy stride, tym większy przeskok i większa odległość między rozmieszczanymi wartościami, co skutkuje większym wyjściem. Kernel size (rozmiar filtra) to wielkość „okienka”, przez które każdy element wejścia jest rozpraszany na wyjściu — im większy kernel, tym większy wpływ pojedynczego piksela na okolicę w macierzy wyjściowej. Padding w transponowanej konwolucji odpowiada za obcinanie brzegów na końcu operacji, żeby dopasować rozmiar wyjścia — jeśli padding jest większy, to finalna macierz wyjściowa będzie mniejsza, bo obcinamy więcej na brzegach. W skrócie: stride i kernel size zwiększają rozmiar wyjścia, a padding pozwala go precyzyjnie przyciąć."
      ],
      "metadata": {
        "id": "EpYiKGrq4k2c"
      },
      "id": "EpYiKGrq4k2c"
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "gpuType": "T4",
      "include_colab_link": true
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}